{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1680692146763,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "pO82uV5UhWZo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tfPeXLWiHwL"
   },
   "source": [
    "### Data  & SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255001, 20)\n",
      "Not_Delayed    210001\n",
      "Delayed         45000\n",
      "Name: Delay, dtype: int64\n",
      "\n",
      "(204000, 17) (51001, 17)\n",
      "0    168109\n",
      "1     35891\n",
      "Name: Delay_num, dtype: int64\n",
      "0    41892\n",
      "1     9109\n",
      "Name: Delay_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet('./data/train_preprocess_2-1.parquet')\n",
    "# test = pd.read_parquet('./test.parquet')\n",
    "test = pd.read_parquet('./data/test_preprocess_2-1.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.Delay.value_counts())\n",
    "print()\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(train.drop(columns=['ID', 'Delay', 'Delay_num']), train['Delay_num'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_x.shape, val_x.shape)\n",
    "print(train_y.value_counts())\n",
    "print(val_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8Eu7kqral4J"
   },
   "source": [
    "### 여러 모델 적합\n",
    "Extra Trees Classifier\t\n",
    "Random Forest Classifier\t\n",
    "Light Gradient Boosting Machine\n",
    "Decision Tree Classifier\t\n",
    "Gradient Boosting Classifier\t\n",
    "Ada Boost Classifier\t\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127454,
     "status": "ok",
     "timestamp": 1680681581120,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "VxdyJSR0Yvsq",
    "outputId": "9675fb47-9c8c-42b5-f596-78a230402b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier: Log Loss = 0.5019\n",
      "Random Forest Classifier: Log Loss = 0.4750\n",
      "Light Gradient Boosting Machine: Log Loss = 0.4424\n",
      "Decision Tree Classifier: Log Loss = 10.4489\n",
      "Gradient Boosting Classifier: Log Loss = 0.4474\n",
      "Ada Boost Classifier: Log Loss = 0.6823\n",
      "Logistic Regression: Log Loss = 0.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# SMOTE 없이 적합\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(random_state=42),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"Light Gradient Boosting Machine\": LGBMClassifier(random_state=42),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Ada Boost Classifier\": AdaBoostClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "}\n",
    "\n",
    "# 각 모델의 성능을 비교\n",
    "for name, model in models.items():\n",
    "    model.fit(train_x, train_y)\n",
    "    y_pred = model.predict_proba(val_x)\n",
    "    loss = log_loss(val_y, y_pred)\n",
    "    print(f\"{name}: Log Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk-CQTW8a-fZ",
    "outputId": "8922d14d-3100-405e-e7fe-8c246d23d0f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier: Log Loss = 0.5167\n",
      "Random Forest Classifier: Log Loss = 0.5130\n",
      "Light Gradient Boosting Machine: Log Loss = 0.5160\n",
      "Decision Tree Classifier: Log Loss = 11.7125\n",
      "Gradient Boosting Classifier: Log Loss = 0.5985\n",
      "Ada Boost Classifier: Log Loss = 0.6901\n",
      "Logistic Regression: Log Loss = 0.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# SMOTE 이용해서 적합\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# SMOTE로 데이터 불균형 완화\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(train_x, train_y)\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(random_state=42),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"Light Gradient Boosting Machine\": LGBMClassifier(random_state=42),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Ada Boost Classifier\": AdaBoostClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "}\n",
    "\n",
    "# 각 모델의 성능을 비교\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict_proba(val_x)\n",
    "    loss = log_loss(val_y, y_pred)\n",
    "    print(f\"{name}: Log Loss = {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOt+IlY3oU3dEkCKHi2bVBw",
   "machine_shape": "hm",
   "mount_file_id": "1IgBELre5zixY8eogePhZKWB_GlZpWi2H",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
