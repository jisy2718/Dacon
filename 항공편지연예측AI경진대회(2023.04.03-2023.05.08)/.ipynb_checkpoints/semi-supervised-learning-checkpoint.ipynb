{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치를 직접 나눠서 원핫인코딩 시, 메모리초과 안나도록!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ03gjSAWexe"
   },
   "source": [
    "## 1. 결측치처리\n",
    "**해당 노트북**\n",
    "+ 전처리방법2 + x결측치삭제 + vae 활용 + validation set 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pO82uV5UhWZo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9midQA40kaZ7"
   },
   "source": [
    "### 1.1. 전처리방법2 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2232,
     "status": "ok",
     "timestamp": 1682398048619,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "R1yJwPUBkaZ9",
    "outputId": "565b249d-cfde-414b-cb1f-f48c303bda5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 19)\n",
      "Not_Delayed    210001\n",
      "Delayed         45000\n",
      "Name: Delay, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet('./data/train_preprocess_2.parquet')\n",
    "# test = pd.read_parquet('./test.parquet')\n",
    "test = pd.read_parquet('./data/test_preprocess_2.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.Delay.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FjWnl6WuVXp"
   },
   "source": [
    "### 1.2. 남은 결측치 처리 - 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2073,
     "status": "ok",
     "timestamp": 1682398467992,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "-_OkLK6bsdCe",
    "outputId": "e5d0d96b-93ce-48e3-a637-9385d0102530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                               0\n",
      "Month                            0\n",
      "Day_of_Month                     0\n",
      "Estimated_Departure_Time         0\n",
      "Estimated_Arrival_Time           0\n",
      "Cancelled                        0\n",
      "Diverted                         0\n",
      "Origin_Airport                   0\n",
      "Origin_Airport_ID                0\n",
      "Origin_State                     0\n",
      "Destination_Airport              0\n",
      "Destination_Airport_ID           0\n",
      "Destination_State                0\n",
      "Distance                         0\n",
      "Airline                          0\n",
      "Carrier_Code(IATA)               0\n",
      "Carrier_ID(DOT)                  0\n",
      "Tail_Number                      0\n",
      "Delay                       520399\n",
      "dtype: int64\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# print(train.isnull().sum())\n",
    "# print(train.dropna().shape)\n",
    "# print(train.dropna().isnull().sum())\n",
    "train = train.dropna(subset=['Estimated_Departure_Time','Estimated_Arrival_Time','Carrier_Code(IATA)','Airline','Carrier_ID(DOT)'])\n",
    "print(train.isnull().sum())\n",
    "\n",
    "\n",
    "# 레이블(Delay)을 제외한 결측값이 존재하는 변수들을 unknown으로 대체합니다.\n",
    "NaN_col = ['Origin_State','Destination_State','Airline','Estimated_Departure_Time', 'Estimated_Arrival_Time','Carrier_Code(IATA)','Carrier_ID(DOT)']\n",
    "\n",
    "# for col in NaN_col:\n",
    "    # mode = train[col].mode()[0]\n",
    "    # train[col] = train[col].fillna(mode)\n",
    "    \n",
    "#     if col in test.columns:\n",
    "#         test[col] = test[col].fillna('Unknown')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 column 생성\n",
    "# train['Estimated_Duration'] = train['Estimated_Arrival_Time'] -  train['Estimated_Departure_Time']\n",
    "# test['Estimated_Duration'] = test['Estimated_Arrival_Time'] -  test['Estimated_Departure_Time']\n",
    "test['Tail_Number'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. label & unlabel split  / label_train & label_validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1배치 데이터 흐름\n",
    "1. vae에는 X_train_labeled와 X_unlabeled를 각각 onehot으로 만들어서 합쳐서 넣어주기\n",
    "2. classifier에는 X_train_labeled를 onehot으로 만든 것 넣어주기\n",
    "\n",
    "\n",
    "#### 필요한 것\n",
    "1. labeled와 unlabeled 나누기\n",
    "2. labeled에서 train과 validation 분리하기\n",
    "3. X_train_labeld & X_unlabeled 를 이용한 onehot encoding\n",
    "4. 전체 데이터에 onehot 적용하면 데이터 크기 너무 커지므로, 배치로 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178176, 17) (520399, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3764\\2020010435.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_labeled['Delay'] = y_labeled['Delay'].apply(lambda x : change_cate2num[x])\n"
     ]
    }
   ],
   "source": [
    "# 1. labeled & unlabeld split\n",
    "train_labeled , train_unlabeled = train[train['Delay'].notnull()], train[train['Delay'].isnull()]\n",
    "\n",
    "X_labeled, y_labeled = train_labeled.drop(['ID','Delay'], axis=1), train_labeled[['Delay']]\n",
    "change_cate2num = {'Not_Delayed':0, \"Delayed\":1}\n",
    "y_labeled['Delay'] = y_labeled['Delay'].apply(lambda x : change_cate2num[x])\n",
    "X_unlabeled = train_unlabeled.drop(['ID','Delay'], axis=1)\n",
    "\n",
    "print(X_labeled.shape, X_unlabeled.shape)\n",
    "\n",
    "\n",
    "# 2. train_labeled & val_labeled split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_labeled, X_val_labeled, y_train_labeled, y_val_labeled = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 3. X_unlabled 의 크기를 X_train_labeled와 맞춰주기\n",
    "# X_unlabeled = X_unlabeled.iloc[:len(X_train_labeled),:]\n",
    "# print(X_unlabeled.shape, X_train_labeled.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. encoder 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. 데이터 정리 & onehotencoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', 'Origin_Airport',\n",
    "       'Origin_Airport_ID', 'Origin_State', 'Destination_Airport',\n",
    "       'Destination_Airport_ID', 'Destination_State', 'Airline',\n",
    "       'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "\n",
    "\n",
    "# Airport 2개 삭제함\n",
    "cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', \n",
    "       'Origin_Airport_ID', 'Origin_State', \n",
    "       'Destination_Airport_ID', 'Destination_State', 'Airline',\n",
    "       'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "\n",
    "# cate_cols = ['Month', 'Day_of_Month',\n",
    "#        'Origin_Airport_ID',\n",
    "#        'Destination_Airport_ID', \n",
    "#              'Airline']\n",
    "\n",
    "numeric_cols = ['Estimated_Departure_Time','Estimated_Arrival_Time','Distance']\n",
    "\n",
    "\n",
    "## 3.1. VAE 훈련에 쓸 데이터 : X_train_labeled, X_unlabeled\n",
    "### 3.1.1. 데이터 정리\n",
    "X_vae_train = pd.concat([X_train_labeled, X_unlabeled])\n",
    "X_vae_train_cate = X_vae_train[cate_cols]\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(X_vae_train_cate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 필요한 것 : X_train_labeled, y_train_labeled, X_vae_train, cate_cols, numeric_cols\n",
    "def make_label_batch(batch_num, num_of_batch):\n",
    "    # 1. 현재 배치 가져오기 - 추후 X_train_labeled를 섞는 과정도 필요\n",
    "    n = len(X_train_labeled)\n",
    "    start_loc = n//num_of_batch*(batch_num-1)\n",
    "    end_loc = n//num_of_batch*batch_num\n",
    "    \n",
    "    X_cur_batch = X_train_labeled.iloc[start_loc:end_loc]\n",
    "    y_cur_batch = y_train_labeled.iloc[start_loc:end_loc]\n",
    "    \n",
    "    # 2. category onehot으로 변환하기\n",
    "    X_sample_category = X_cur_batch[cate_cols]\n",
    "    X_sample_category = encoder.transform(X_sample_category)\n",
    "    X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "    \n",
    "    # 3. numeric은 0~1 사이로 바꿔주기\n",
    "    X_sample_numeric = X_cur_batch[numeric_cols]\n",
    "    max_values = X_vae_train[numeric_cols].max()\n",
    "    X_sample_numeric = np.array(X_sample_numeric /max_values.values) # 0~1사이로 변환\n",
    "#     print(X_sample_category.shape, X_sample_numeric.shape)\n",
    "    \n",
    "    # 4. category & numeric 합치기\n",
    "    X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "\n",
    "    # 5. 텐서로 변환하기 : (batch_size, column dim)\n",
    "    X_sample = torch.tensor(X_sample, dtype=torch.float32)  \n",
    "    y_sample = torch.tensor(y_cur_batch.values, dtype=torch.float32)\n",
    "    \n",
    "    return X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 것 : X_unlabeled, X_vae_train, cate_cols, numeric_cols\n",
    "def make_unlabel_batch(batch_num,num_of_batch):\n",
    "    # 1. 현재 배치 가져오기 - 추후 X_unlabeled 섞는 과정도 필요\n",
    "    n = len(X_train_labeled) # 임시방편\n",
    "    start_loc = n//num_of_batch*(batch_num-1)\n",
    "    end_loc = n//num_of_batch*batch_num\n",
    "    \n",
    "    X_cur_batch = X_unlabeled.iloc[start_loc:end_loc]\n",
    "    \n",
    "    # 2. category onehot으로 변환하기\n",
    "    X_sample_category = X_cur_batch[cate_cols]\n",
    "    X_sample_category = encoder.transform(X_sample_category)\n",
    "    X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "    \n",
    "    # 3. numeric은 0~1 사이로 바꿔주기\n",
    "    X_sample_numeric = X_cur_batch[numeric_cols]\n",
    "    max_values = X_vae_train[numeric_cols].max()\n",
    "    X_sample_numeric = np.array(X_sample_numeric /max_values.values) # 0~1사이로 변환\n",
    "#     print(X_sample_category.shape, X_sample_numeric.shape)\n",
    "    \n",
    "    # 4. category & numeric 합치기\n",
    "    X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "\n",
    "    # 5. 텐서로 변환하기 : (batch_size, column dim)\n",
    "    X_sample = torch.tensor(X_sample, dtype=torch.float32)  \n",
    "    \n",
    "    return X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation X를 tensor로 만들기\n",
    "def make_validation_tensor():\n",
    "    X_cur_batch = X_val_labeled.copy()\n",
    "    \n",
    "    # 2. category onehot으로 변환하기\n",
    "    X_sample_category = X_cur_batch[cate_cols]\n",
    "    X_sample_category = encoder.transform(X_sample_category)\n",
    "    X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "    \n",
    "    # 3. numeric은 0~1 사이로 바꿔주기\n",
    "    X_sample_numeric = X_cur_batch[numeric_cols]\n",
    "    max_values = X_vae_train[numeric_cols].max()\n",
    "    X_sample_numeric = np.array(X_sample_numeric /max_values.values) # 0~1사이로 변환\n",
    "#     print(X_sample_category.shape, X_sample_numeric.shape)\n",
    "    \n",
    "    # 4. category & numeric 합치기\n",
    "    X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "\n",
    "    # 5. 텐서로 변환하기 : (batch_size, column dim)\n",
    "    X_sample = torch.tensor(X_sample, dtype=torch.float32)  \n",
    "    \n",
    "    return X_sample    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35636, 7377])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_labeled_tensor = make_validation_tensor()\n",
    "X_val_labeled_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_Month</th>\n",
       "      <th>Estimated_Departure_Time</th>\n",
       "      <th>Estimated_Arrival_Time</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>Origin_Airport</th>\n",
       "      <th>Origin_Airport_ID</th>\n",
       "      <th>Origin_State</th>\n",
       "      <th>Destination_Airport</th>\n",
       "      <th>Destination_Airport_ID</th>\n",
       "      <th>Destination_State</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Carrier_Code(IATA)</th>\n",
       "      <th>Carrier_ID(DOT)</th>\n",
       "      <th>Tail_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900000</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>1759.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DTW</td>\n",
       "      <td>11433</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>MSN</td>\n",
       "      <td>13485</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>311.0</td>\n",
       "      <td>SkyWest Airlines Inc.</td>\n",
       "      <td>DL</td>\n",
       "      <td>20304.0</td>\n",
       "      <td>N803SK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900001</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IAH</td>\n",
       "      <td>12266</td>\n",
       "      <td>Texas</td>\n",
       "      <td>LGA</td>\n",
       "      <td>12953</td>\n",
       "      <td>New York</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977.0</td>\n",
       "      <td>N23707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900002</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SJU</td>\n",
       "      <td>14843</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ORD</td>\n",
       "      <td>13930</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>19977.0</td>\n",
       "      <td>N45440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900003</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SFO</td>\n",
       "      <td>14771</td>\n",
       "      <td>California</td>\n",
       "      <td>DEN</td>\n",
       "      <td>11292</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>967.0</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N66841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900004</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1827.0</td>\n",
       "      <td>2346.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PHX</td>\n",
       "      <td>14107</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>MDW</td>\n",
       "      <td>13232</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N8669B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LAX</td>\n",
       "      <td>12892</td>\n",
       "      <td>California</td>\n",
       "      <td>DEN</td>\n",
       "      <td>11292</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>862.0</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>WN</td>\n",
       "      <td>19393.0</td>\n",
       "      <td>N720WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>600.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BUF</td>\n",
       "      <td>10792</td>\n",
       "      <td>New York</td>\n",
       "      <td>ORD</td>\n",
       "      <td>13930</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>473.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977.0</td>\n",
       "      <td>N401UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IAD</td>\n",
       "      <td>12264</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>DTW</td>\n",
       "      <td>11433</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>383.0</td>\n",
       "      <td>Mesa Airlines Inc.</td>\n",
       "      <td>UA</td>\n",
       "      <td>20378.0</td>\n",
       "      <td>N510MJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>14679</td>\n",
       "      <td>California</td>\n",
       "      <td>BOS</td>\n",
       "      <td>10721</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2588.0</td>\n",
       "      <td>JetBlue Airways</td>\n",
       "      <td>B6</td>\n",
       "      <td>20409.0</td>\n",
       "      <td>N986JB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OAK</td>\n",
       "      <td>13796</td>\n",
       "      <td>California</td>\n",
       "      <td>LAX</td>\n",
       "      <td>12892</td>\n",
       "      <td>California</td>\n",
       "      <td>337.0</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>WN</td>\n",
       "      <td>19393.0</td>\n",
       "      <td>N733SA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Month  Day_of_Month  Estimated_Departure_Time  Estimated_Arrival_Time  \\\n",
       "900000      5            24                    1740.0                  1759.0   \n",
       "900001      7            17                    1220.0                  1655.0   \n",
       "900002      5             8                    1425.0                  1835.0   \n",
       "900003      3            13                    1444.0                  1814.0   \n",
       "900004      3            13                    1827.0                  2346.0   \n",
       "...       ...           ...                       ...                     ...   \n",
       "999995      6             2                       NaN                  2240.0   \n",
       "999996      6             8                     600.0                   648.0   \n",
       "999997      6            11                    1710.0                  1907.0   \n",
       "999998     11            17                       NaN                   500.0   \n",
       "999999     10            13                    1220.0                  1345.0   \n",
       "\n",
       "        Cancelled  Diverted Origin_Airport  Origin_Airport_ID Origin_State  \\\n",
       "900000          0         0            DTW              11433     Michigan   \n",
       "900001          0         0            IAH              12266        Texas   \n",
       "900002          0         0            SJU              14843  Puerto Rico   \n",
       "900003          0         0            SFO              14771   California   \n",
       "900004          0         0            PHX              14107      Arizona   \n",
       "...           ...       ...            ...                ...          ...   \n",
       "999995          0         0            LAX              12892   California   \n",
       "999996          0         0            BUF              10792     New York   \n",
       "999997          0         0            IAD              12264     Virginia   \n",
       "999998          0         0            SAN              14679   California   \n",
       "999999          0         0            OAK              13796   California   \n",
       "\n",
       "       Destination_Airport  Destination_Airport_ID Destination_State  \\\n",
       "900000                 MSN                   13485         Wisconsin   \n",
       "900001                 LGA                   12953          New York   \n",
       "900002                 ORD                   13930          Illinois   \n",
       "900003                 DEN                   11292          Colorado   \n",
       "900004                 MDW                   13232          Illinois   \n",
       "...                    ...                     ...               ...   \n",
       "999995                 DEN                   11292          Colorado   \n",
       "999996                 ORD                   13930          Illinois   \n",
       "999997                 DTW                   11433          Michigan   \n",
       "999998                 BOS                   10721     Massachusetts   \n",
       "999999                 LAX                   12892        California   \n",
       "\n",
       "        Distance                 Airline Carrier_Code(IATA)  Carrier_ID(DOT)  \\\n",
       "900000     311.0   SkyWest Airlines Inc.                 DL          20304.0   \n",
       "900001    1416.0   United Air Lines Inc.                 UA          19977.0   \n",
       "900002    2072.0   United Air Lines Inc.               None          19977.0   \n",
       "900003     967.0                    None                 UA              NaN   \n",
       "900004    1444.0                    None               None              NaN   \n",
       "...          ...                     ...                ...              ...   \n",
       "999995     862.0  Southwest Airlines Co.                 WN          19393.0   \n",
       "999996     473.0   United Air Lines Inc.                 UA          19977.0   \n",
       "999997     383.0      Mesa Airlines Inc.                 UA          20378.0   \n",
       "999998    2588.0         JetBlue Airways                 B6          20409.0   \n",
       "999999     337.0  Southwest Airlines Co.                 WN          19393.0   \n",
       "\n",
       "       Tail_Number  \n",
       "900000      N803SK  \n",
       "900001      N23707  \n",
       "900002      N45440  \n",
       "900003      N66841  \n",
       "900004      N8669B  \n",
       "...            ...  \n",
       "999995      N720WN  \n",
       "999996      N401UA  \n",
       "999997      N510MJ  \n",
       "999998      N986JB  \n",
       "999999      N733SA  \n",
       "\n",
       "[100000 rows x 17 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[900000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_batch(batch_num,num_of_batch):\n",
    "    n = len(X_test) # 임시방편\n",
    "    start_loc = n//num_of_batch*(batch_num-1)\n",
    "    end_loc = n//num_of_batch*batch_num \n",
    "    \n",
    "    X_cur_batch = X_test.iloc[start_loc:end_loc] if batch_num!= num_of_batch else X_test.iloc[start_loc:]\n",
    "    \n",
    "    # 2. category onehot으로 변환하기\n",
    "    X_sample_category = X_cur_batch[cate_cols]\n",
    "    X_sample_category = encoder.transform(X_sample_category)\n",
    "    X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "    \n",
    "    # 3. numeric은 0~1 사이로 바꿔주기 & Nan 값 0으로 처리\n",
    "    X_sample_numeric = X_cur_batch[numeric_cols].fillna(0)\n",
    "    max_values = X_vae_train[numeric_cols].max()\n",
    "    X_sample_numeric = np.array(X_sample_numeric /max_values.values) # 0~1사이로 변환\n",
    "#     print(X_sample_category.shape, X_sample_numeric.shape)\n",
    "    \n",
    "    # 4. category & numeric 합치기\n",
    "    X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "\n",
    "    # 5. 텐서로 변환하기 : (batch_size, column dim)\n",
    "    X_sample = torch.tensor(X_sample, dtype=torch.float32)  \n",
    "    \n",
    "    return X_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기 생성\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "#             nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    \n",
    "    # 분류기 생성\n",
    "class Classifier_drop(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier_drop, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "#             nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE LOSS\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def vae_loss(reconstructed_data, original_data, mu, log_var):\n",
    "#     # Reconstruction Loss\n",
    "#     recon_loss = F.binary_cross_entropy(reconstructed_data, original_data, reduction='mean')\n",
    "\n",
    "#     # KL Divergence Loss\n",
    "#     kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "#     # Total Loss\n",
    "#     total_loss = recon_loss + kl_divergence\n",
    "#     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  7377\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X_val_labeled_tensor.shape[1]\n",
    "print('input shape : ', X_val_labeled_tensor.shape[1])\n",
    "hidden_dim = 256\n",
    "latent_dim = 6\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, optimizer and loss function\n",
    "device = 'cuda:0'\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "vae.to(device)\n",
    "\n",
    "reconstruction_loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "\n",
    "# classifier 초기화\n",
    "classifier = Classifier_drop(input_dim = latent_dim, hidden_dim = latent_dim//2 , output_dim= 2)\n",
    "classifier.to(device)\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# optimizer\n",
    "combined_parameters = list(vae.parameters()) + list(classifier.parameters())\n",
    "optimizer = optim.Adam(combined_parameters, lr=learning_rate)\n",
    "\n",
    "\n",
    "# vae.load_state_dict(torch.load('./VAE_new.pth'))\n",
    "# classifier.load_state_dict(torch.load('./classifier_new.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델훈련\n",
    "현재 갑자기 training이 안되는 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, total: 2.9421, labeled_loss: 1.1795, unlabeled_loss: 1.1790, class_loss:  0.5836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▊                                                                                | 1/30 [01:01<29:41, 61.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.6079\n",
      "epoch:2, total: 0.6587, labeled_loss: 0.0673, unlabeled_loss: 0.0679, class_loss:  0.5234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                             | 2/30 [02:02<28:37, 61.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.5741\n",
      "epoch:3, total: 0.5288, labeled_loss: 0.0325, unlabeled_loss: 0.0326, class_loss:  0.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 3/30 [03:03<27:33, 61.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.5412\n",
      "epoch:4, total: 0.4471, labeled_loss: 0.0214, unlabeled_loss: 0.0214, class_loss:  0.4043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████                                                                        | 4/30 [04:05<26:33, 61.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.5142\n",
      "epoch:5, total: 0.3866, labeled_loss: 0.0162, unlabeled_loss: 0.0162, class_loss:  0.3542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▊                                                                     | 5/30 [05:06<25:31, 61.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.4981\n",
      "epoch:6, total: 0.3412, labeled_loss: 0.0135, unlabeled_loss: 0.0135, class_loss:  0.3141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 6/30 [06:08<24:36, 61.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.4888\n",
      "epoch:7, total: 0.3062, labeled_loss: 0.0119, unlabeled_loss: 0.0119, class_loss:  0.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|███████████████████▎                                                               | 7/30 [07:09<23:34, 61.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.4825\n",
      "epoch:8, total: 0.2781, labeled_loss: 0.0108, unlabeled_loss: 0.0108, class_loss:  0.2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██████████████████████▏                                                            | 8/30 [08:10<22:29, 61.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.4820\n",
      "epoch:9, total: 0.2546, labeled_loss: 0.0101, unlabeled_loss: 0.0101, class_loss:  0.2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 9/30 [09:12<21:26, 61.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.4829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 9/30 [09:35<22:21, 63.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Labeled data에 대한 VAE 훈련\u001b[39;00m\n\u001b[0;32m     29\u001b[0m reconstructed_labeled_data, mu, log_var \u001b[38;5;241m=\u001b[39m vae(labeled_data\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 30\u001b[0m labeled_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss(reconstructed_labeled_data, \u001b[43mlabeled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m log_var \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m log_var\u001b[38;5;241m.\u001b[39mexp())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Unlabeled data에 대한 VAE 훈련\u001b[39;00m\n\u001b[0;32m     33\u001b[0m reconstructed_unlabeled_data, mu_unlabeled, log_var_unlabeled \u001b[38;5;241m=\u001b[39m vae(unlabeled_data\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device = 'cuda:0'\n",
    "vae.to(device)\n",
    "classifier.to(device)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    vae.train()\n",
    "    classifier.train()\n",
    "    # epoch 마다 data shuffle\n",
    "    tmp = pd.concat([X_train_labeled, y_train_labeled], axis=1)\n",
    "    tmp = tmp.sample(frac=1).reset_index(drop=True)\n",
    "    X_train_labeled = tmp.drop(columns=['Delay'])\n",
    "    y_train_labeled = tmp[['Delay']]\n",
    "    \n",
    "    # epoch 마다 loss 세기\n",
    "    loss_dict = {'total_loss':0, 'labeled_loss':0, 'unlabeled_loss':0, 'class_loss':0}\n",
    "    num_of_batch = 50\n",
    "    for batch_num in range(1,num_of_batch+1):\n",
    "        labeled_data, labeled_label = make_label_batch(batch_num, num_of_batch)\n",
    "        unlabeled_data = make_unlabel_batch(batch_num, num_of_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Labeled data에 대한 VAE 훈련\n",
    "        reconstructed_labeled_data, mu, log_var = vae(labeled_data.to(device))\n",
    "        labeled_loss = reconstruction_loss(reconstructed_labeled_data, labeled_data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Unlabeled data에 대한 VAE 훈련\n",
    "        reconstructed_unlabeled_data, mu_unlabeled, log_var_unlabeled = vae(unlabeled_data.to(device))\n",
    "        unlabeled_loss = reconstruction_loss(reconstructed_unlabeled_data, unlabeled_data.to(device)) - 0.5 * torch.sum(1 + log_var_unlabeled - mu_unlabeled.pow(2) - log_var_unlabeled.exp())\n",
    "\n",
    "        # Labeled data에 대한 Classifier 훈련\n",
    "        latent_labeled_data = vae.reparameterize(mu, log_var)\n",
    "        classifier_output = classifier(latent_labeled_data)\n",
    "        labeled_label_indices = torch.argmax(labeled_label, dim=1).to(device) # 다중 타겟을 원핫으로 변환\n",
    "        class_loss = classification_loss(classifier_output, labeled_label_indices)\n",
    "\n",
    "        # 총 Loss 계산 및 업데이트\n",
    "        total_loss = labeled_loss + unlabeled_loss + class_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        loss_dict['total_loss'] += total_loss.item()\n",
    "        loss_dict['labeled_loss'] += labeled_loss.item()\n",
    "        loss_dict['unlabeled_loss'] += unlabeled_loss.item()\n",
    "        loss_dict['class_loss'] += class_loss.item()\n",
    "\n",
    "#         if batch_num % 10 ==0:\n",
    "#             print(batch_num)\n",
    "\n",
    "    print(f\"epoch:{epoch+1}, total:{loss_dict['total_loss']/num_of_batch : .4f}, labeled_loss:{loss_dict['labeled_loss']/num_of_batch : .4f}, unlabeled_loss:{loss_dict['unlabeled_loss']/num_of_batch : .4f}, class_loss: {loss_dict['class_loss']/num_of_batch : .4f}\")\n",
    "    scheduler.step()\n",
    "    # validation 점수 측정\n",
    "    validation_score()\n",
    "    torch.save(vae.state_dict(), f'./VAE _{epoch+1}.pth') # 모델 저장\n",
    "    torch.save(classifier.state_dict(), f'./classifier_dropout_{epoch+1}.pth') # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vae.state_dict(), './VAE2.pth') # 모델 저장\n",
    "# torch.save(classifier.state_dict(), './classifier_dropout1.pth') # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13988\\4099449297.py:43: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_labeled_le, y_train_labeled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier: Log Loss = 0.5265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13988\\4099449297.py:43: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_labeled_le, y_train_labeled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier: Log Loss = 0.4804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light Gradient Boosting Machine: Log Loss = 0.4399\n",
      "Decision Tree Classifier: Log Loss = 10.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier: Log Loss = 0.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Classifier: Log Loss = 0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Log Loss = 0.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 1. train 이용한 경우 validation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# 데이터 수치화\n",
    "qual_col = ['Origin_Airport', 'Origin_State', 'Destination_Airport', 'Destination_State', 'Airline', 'Carrier_Code(IATA)', 'Tail_Number']\n",
    "X_train_labeled_le = X_train_labeled.copy()\n",
    "X_val_labeled_le = X_val_labeled.copy()\n",
    "test_le = test.copy()\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le=le.fit(train[i]) # 사용할 수 있는 전체 X를 이용해서 LE\n",
    "    X_train_labeled_le[i]=le.transform(X_train_labeled_le[i])\n",
    "    X_val_labeled_le[i]=le.transform(X_val_labeled_le[i])\n",
    "    \n",
    "    for label in np.unique(test[i]):\n",
    "        if label not in le.classes_: # train에 없는 label인 경우\n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test_le[i]=le.transform(test_le[i])\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(random_state=42),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"Light Gradient Boosting Machine\": LGBMClassifier(random_state=42),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Ada Boost Classifier\": AdaBoostClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "}\n",
    "\n",
    "# 각 모델의 성능을 비교\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_labeled_le, y_train_labeled)\n",
    "    y_pred = model.predict_proba(X_val_labeled_le)\n",
    "    loss = log_loss(y_val_labeled, y_pred)\n",
    "    print(f\"{name}: Log Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Log Loss = 0.6505\n"
     ]
    }
   ],
   "source": [
    "# 2. semi-supervised 된 신경망 모델의 validation set 예측성능\n",
    "from sklearn.metrics import log_loss\n",
    "def validation_score():\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        classifier.eval()\n",
    "\n",
    "        ## 잠재벡터화\n",
    "        reconstructed_labeled_data, mu, log_var = vae(X_val_labeled_tensor.to(device))\n",
    "        X_val_latent_labeled_data = vae.reparameterize(mu, log_var)\n",
    "\n",
    "        ## 예측\n",
    "        y_pred = classifier(X_val_latent_labeled_data)\n",
    "        loss = log_loss(y_val_labeled, torch.softmax(y_pred,dim=1).cpu().detach().numpy()) # softmax로 확률로 바꿔줘야!\n",
    "        print(f\" Validation Log Loss = {loss:.4f}\")\n",
    "#         print(torch.softmax(y_pred,dim=1)[:20,:])\n",
    "validation_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "X_test = test.drop(columns=['ID'])\n",
    "test_batch = make_test_batch(1,10)\n",
    "\n",
    "# 예측하기\n",
    "device = 'cuda:2'\n",
    "with torch.no_grad():\n",
    "    vae.to(device)\n",
    "    classifier.to(device)\n",
    "    vae.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    ## 잠재벡터화\n",
    "    reconstructed_labeled_data, mu, log_var = vae(test_batch.to(device))\n",
    "    X_test_latent_labeled_data = vae.reparameterize(mu, log_var)\n",
    "\n",
    "    ## 예측\n",
    "    y_pred = classifier(X_test_latent_labeled_data)\n",
    "    y_pred = torch.softmax(y_pred,dim=1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num :  2\n",
      "batch num :  3\n",
      "batch num :  4\n",
      "batch num :  5\n",
      "batch num :  6\n",
      "batch num :  7\n",
      "batch num :  8\n",
      "batch num :  9\n",
      "batch num :  10\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "# X_test = test.drop(columns=['ID'])\n",
    "for batch_num in range(2, 11):\n",
    "    test_batch = make_test_batch(batch_num,10)\n",
    "\n",
    "    # 예측하기\n",
    "    device = 'cuda:2'\n",
    "    with torch.no_grad():\n",
    "        vae.to(device)\n",
    "        classifier.to(device)\n",
    "        vae.eval()\n",
    "        classifier.eval()\n",
    "\n",
    "        ## 잠재벡터화\n",
    "        reconstructed_labeled_data, mu, log_var = vae(test_batch.to(device))\n",
    "        X_test_latent_labeled_data = vae.reparameterize(mu, log_var)\n",
    "\n",
    "        ## 예측\n",
    "        y_pred_batch_num = classifier(X_test_latent_labeled_data)\n",
    "        y_pred_batch_num = torch.softmax(y_pred_batch_num,dim=1).cpu().detach().numpy()\n",
    "    y_pred = np.vstack([y_pred, y_pred_batch_num])\n",
    "    print('batch num : ', batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출본 만들기\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_semi-supervise-vae2.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjmnbSg0kabA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
