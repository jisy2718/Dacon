{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28104,
     "status": "ok",
     "timestamp": 1682395271298,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "-shg-k69keOm",
    "outputId": "8b8ee30a-2cdc-416b-b7cd-9341bd75000c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1682395300054,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "WLDRz-uvkgjL",
    "outputId": "9f8c557a-ef34-4e78-f7dd-80e703cb5f9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/github/Dacon/항공편지연예측AI경진대회(2023.04.03-2023.05.08)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/github/Dacon/항공편지연예측AI경진대회(2023.04.03-2023.05.08)')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ03gjSAWexe"
   },
   "source": [
    "## 1. 결측치처리\n",
    "**해당 노트북**\n",
    "+ 전처리방법2 + x결측치삭제 + vae 활용 + validation set 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pO82uV5UhWZo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9midQA40kaZ7"
   },
   "source": [
    "### 1.1. 전처리방법2 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2232,
     "status": "ok",
     "timestamp": 1682398048619,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "R1yJwPUBkaZ9",
    "outputId": "565b249d-cfde-414b-cb1f-f48c303bda5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 19)\n",
      "Not_Delayed    210001\n",
      "Delayed         45000\n",
      "Name: Delay, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet('./data/train_preprocess_2.parquet')\n",
    "# test = pd.read_parquet('./test.parquet')\n",
    "test = pd.read_parquet('./data/test_preprocess_2.parquet')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.Delay.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FjWnl6WuVXp"
   },
   "source": [
    "### 1.2. 남은 결측치 처리 - 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2073,
     "status": "ok",
     "timestamp": 1682398467992,
     "user": {
      "displayName": "지승영",
      "userId": "07006377205744982370"
     },
     "user_tz": -540
    },
    "id": "-_OkLK6bsdCe",
    "outputId": "e5d0d96b-93ce-48e3-a637-9385d0102530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                               0\n",
      "Month                            0\n",
      "Day_of_Month                     0\n",
      "Estimated_Departure_Time         0\n",
      "Estimated_Arrival_Time           0\n",
      "Cancelled                        0\n",
      "Diverted                         0\n",
      "Origin_Airport                   0\n",
      "Origin_Airport_ID                0\n",
      "Origin_State                     0\n",
      "Destination_Airport              0\n",
      "Destination_Airport_ID           0\n",
      "Destination_State                0\n",
      "Distance                         0\n",
      "Airline                          0\n",
      "Carrier_Code(IATA)               0\n",
      "Carrier_ID(DOT)                  0\n",
      "Tail_Number                      0\n",
      "Delay                       520399\n",
      "dtype: int64\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# print(train.isnull().sum())\n",
    "# print(train.dropna().shape)\n",
    "# print(train.dropna().isnull().sum())\n",
    "train = train.dropna(subset=['Estimated_Departure_Time','Estimated_Arrival_Time','Carrier_Code(IATA)','Airline','Carrier_ID(DOT)'])\n",
    "print(train.isnull().sum())\n",
    "\n",
    "\n",
    "# 레이블(Delay)을 제외한 결측값이 존재하는 변수들을 unknown으로 대체합니다.\n",
    "NaN_col = ['Origin_State','Destination_State','Airline','Estimated_Departure_Time', 'Estimated_Arrival_Time','Carrier_Code(IATA)','Carrier_ID(DOT)']\n",
    "\n",
    "for col in NaN_col:\n",
    "    # mode = train[col].mode()[0]\n",
    "    # train[col] = train[col].fillna(mode)\n",
    "    \n",
    "    if col in test.columns:\n",
    "        test[col] = test[col].fillna('Unknown')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_Month</th>\n",
       "      <th>Estimated_Departure_Time</th>\n",
       "      <th>Estimated_Arrival_Time</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>Origin_Airport</th>\n",
       "      <th>Origin_Airport_ID</th>\n",
       "      <th>Origin_State</th>\n",
       "      <th>Destination_Airport</th>\n",
       "      <th>Destination_Airport_ID</th>\n",
       "      <th>Destination_State</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Carrier_Code(IATA)</th>\n",
       "      <th>Carrier_ID(DOT)</th>\n",
       "      <th>Tail_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IAH</td>\n",
       "      <td>12266</td>\n",
       "      <td>Texas</td>\n",
       "      <td>SAT</td>\n",
       "      <td>14683</td>\n",
       "      <td>Texas</td>\n",
       "      <td>191.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977.0</td>\n",
       "      <td>N79402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>11618</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>ATL</td>\n",
       "      <td>10397</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>746.0</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "      <td>DL</td>\n",
       "      <td>19790.0</td>\n",
       "      <td>N3765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ORD</td>\n",
       "      <td>13930</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>LGA</td>\n",
       "      <td>12953</td>\n",
       "      <td>New York</td>\n",
       "      <td>733.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "      <td>UA</td>\n",
       "      <td>19977.0</td>\n",
       "      <td>N413UA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>2045.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OAK</td>\n",
       "      <td>13796</td>\n",
       "      <td>California</td>\n",
       "      <td>LAX</td>\n",
       "      <td>12892</td>\n",
       "      <td>California</td>\n",
       "      <td>337.0</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>WN</td>\n",
       "      <td>19393.0</td>\n",
       "      <td>N905WN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>2152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FLL</td>\n",
       "      <td>11697</td>\n",
       "      <td>Florida</td>\n",
       "      <td>LAX</td>\n",
       "      <td>12892</td>\n",
       "      <td>California</td>\n",
       "      <td>2343.0</td>\n",
       "      <td>JetBlue Airways</td>\n",
       "      <td>B6</td>\n",
       "      <td>20409.0</td>\n",
       "      <td>N945JT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Month  Day_of_Month Estimated_Departure_Time  \\\n",
       "0  TEST_000000     12            16                   1156.0   \n",
       "1  TEST_000001      9            12                   1500.0   \n",
       "2  TEST_000002      3             6                   1600.0   \n",
       "3  TEST_000003      5            18                   1920.0   \n",
       "4  TEST_000004      7             7                   1915.0   \n",
       "\n",
       "  Estimated_Arrival_Time  Cancelled  Diverted Origin_Airport  \\\n",
       "0                Unknown          0         0            IAH   \n",
       "1                 1715.0          0         0            EWR   \n",
       "2                 1915.0          0         0            ORD   \n",
       "3                 2045.0          0         0            OAK   \n",
       "4                 2152.0          0         0            FLL   \n",
       "\n",
       "   Origin_Airport_ID Origin_State Destination_Airport  Destination_Airport_ID  \\\n",
       "0              12266        Texas                 SAT                   14683   \n",
       "1              11618   New Jersey                 ATL                   10397   \n",
       "2              13930     Illinois                 LGA                   12953   \n",
       "3              13796   California                 LAX                   12892   \n",
       "4              11697      Florida                 LAX                   12892   \n",
       "\n",
       "  Destination_State  Distance                 Airline Carrier_Code(IATA)  \\\n",
       "0             Texas     191.0   United Air Lines Inc.                 UA   \n",
       "1           Georgia     746.0    Delta Air Lines Inc.                 DL   \n",
       "2          New York     733.0   United Air Lines Inc.                 UA   \n",
       "3        California     337.0  Southwest Airlines Co.                 WN   \n",
       "4        California    2343.0         JetBlue Airways                 B6   \n",
       "\n",
       "  Carrier_ID(DOT) Tail_Number  \n",
       "0         19977.0      N79402  \n",
       "1         19790.0       N3765  \n",
       "2         19977.0      N413UA  \n",
       "3         19393.0      N905WN  \n",
       "4         20409.0      N945JT  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 column 생성\n",
    "# train['Estimated_Duration'] = train['Estimated_Arrival_Time'] -  train['Estimated_Departure_Time']\n",
    "# test['Estimated_Duration'] = test['Estimated_Arrival_Time'] -  test['Estimated_Departure_Time']\n",
    "test['Tail_Number'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. label & unlabel split  / label_train & label_validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1배치 데이터 흐름\n",
    "1. vae에는 X_train_labeled와 X_unlabeled를 각각 onehot으로 만들어서 합쳐서 넣어주기\n",
    "2. classifier에는 X_train_labeled를 onehot으로 만든 것 넣어주기\n",
    "\n",
    "\n",
    "#### 필요한 것\n",
    "1. labeled와 unlabeled 나누기\n",
    "2. labeled에서 train과 validation 분리하기\n",
    "3. X_train_labeld & X_unlabeled 를 이용한 onehot encoding\n",
    "4. 전체 데이터에 onehot 적용하면 데이터 크기 너무 커지므로, 배치로 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178176, 17) (520399, 17)\n",
      "(142540, 17) (142540, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3696\\410560725.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_labeled['Delay'] = y_labeled['Delay'].apply(lambda x : change_cate2num[x])\n"
     ]
    }
   ],
   "source": [
    "# 1. labeled & unlabeld split\n",
    "train_labeled , train_unlabeled = train[train['Delay'].notnull()], train[train['Delay'].isnull()]\n",
    "\n",
    "X_labeled, y_labeled = train_labeled.drop(['ID','Delay'], axis=1), train_labeled[['Delay']]\n",
    "change_cate2num = {'Not_Delayed':0, \"Delayed\":1}\n",
    "y_labeled['Delay'] = y_labeled['Delay'].apply(lambda x : change_cate2num[x])\n",
    "X_unlabeled = train_unlabeled.drop(['ID','Delay'], axis=1)\n",
    "\n",
    "print(X_labeled.shape, X_unlabeled.shape)\n",
    "\n",
    "\n",
    "# 2. train_labeled & val_labeled split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_labeled, X_val_labeled, y_train_labeled, y_val_labeled = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 3. X_unlabled 의 크기를 X_train_labeled와 맞춰주기\n",
    "X_unlabeled = X_unlabeled.iloc[:len(X_train_labeled),:]\n",
    "print(X_unlabeled.shape, X_train_labeled.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. encoder 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. 데이터 정리 & onehotencoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', 'Origin_Airport',\n",
    "       'Origin_Airport_ID', 'Origin_State', 'Destination_Airport',\n",
    "       'Destination_Airport_ID', 'Destination_State', 'Airline',\n",
    "       'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "\n",
    "\n",
    "# Airport 2개 삭제함\n",
    "cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', \n",
    "       'Origin_Airport_ID', 'Origin_State', \n",
    "       'Destination_Airport_ID', 'Destination_State', 'Airline',\n",
    "       'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "\n",
    "cate_cols = ['Month', 'Day_of_Month',\n",
    "       'Origin_Airport_ID',\n",
    "       'Destination_Airport_ID', \n",
    "             'Airline']\n",
    "\n",
    "numeric_cols = ['Estimated_Departure_Time','Estimated_Arrival_Time','Distance']\n",
    "\n",
    "\n",
    "## 3.1. VAE 훈련에 쓸 데이터 : X_train_labeled, X_unlabeled\n",
    "### 3.1.1. 데이터 정리\n",
    "X_vae_train = pd.concat([X_train_labeled, X_unlabeled])\n",
    "X_vae_train_cate = X_vae_train[cate_cols]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(X_vae_train_cate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "\n",
    "class Flight_labeled(Dataset): \n",
    "    def __init__(self, X_train_labeled, y_train_labeled, encoder):\n",
    "        # 1. 데이터 받아오기\n",
    "        self.X_train_labeled = X_train_labeled\n",
    "        self.y_train_labeled =  y_train_labeled\n",
    "        \n",
    "        self.cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', 'Origin_Airport_ID', \\\n",
    "                          'Origin_State', 'Destination_Airport_ID', 'Destination_State', 'Airline',\\\n",
    "                          'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "        \n",
    "        self.cate_cols = cate_cols\n",
    "\n",
    "        self.numeric_cols = ['Estimated_Departure_Time','Estimated_Arrival_Time','Distance']\n",
    "\n",
    "        self.max_values = X_train_labeled[self.numeric_cols].max()\n",
    "        \n",
    "        \n",
    "    # 사용 가능한 데이터 개수 return\n",
    "    def __len__(self):\n",
    "        return len(self.X_train_labeled)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # onehot encoding 후, tensor로 반환하기\n",
    "        # 1. category는 onehot으로 변환하고, numeric은 category onehot 뒤에 붙이기\n",
    "        X_sample_category = self.X_train_labeled[self.cate_cols].iloc[i,:].to_frame().T\n",
    "#         print('1: ', X_sample_category.dtype, X_sample_category.shape)\n",
    "        \n",
    "        X_sample_category = encoder.transform(X_sample_category)\n",
    "#         print('2: ', X_sample_category.dtype, X_sample_category.shape)\n",
    "        \n",
    "        X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "#         print('3: ', X_sample_category.dtype, X_sample_category.shape)\n",
    "        \n",
    "        X_sample_numeric = np.array(self.X_train_labeled[self.numeric_cols].iloc[i,:]).reshape(1,-1)\n",
    "        X_sample_numeric = X_sample_numeric / self.max_values.values.reshape(1,-1)\n",
    "\n",
    "#         print('4: ', X_sample_numeric.dtype, X_sample_numeric.shape)\n",
    "        \n",
    "        X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "#         print('5: ', X_sample.dtype, X_sample.shape)\n",
    "        \n",
    "        # 2. 텐서로 변환하기\n",
    "        X_sample = torch.tensor(X_sample.squeeze(), dtype=torch.float32) # (1, input_dim) -> (input_dim,) 으로 차원 축소\n",
    "        y_sample = torch.tensor(self.y_train_labeled.iloc[i,:].values, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        return X_sample, y_sample\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "class Flight_unlabeled(Dataset):\n",
    "    def __init__(self, X_unlabeled, encoder):\n",
    "        self.X_unlabeled =  X_unlabeled\n",
    "        \n",
    "        self.cate_cols = ['Month', 'Day_of_Month', 'Cancelled', 'Diverted', 'Origin_Airport_ID', \\\n",
    "                          'Origin_State', 'Destination_Airport_ID', 'Destination_State', 'Airline',\\\n",
    "                          'Carrier_Code(IATA)', 'Carrier_ID(DOT)', 'Tail_Number']\n",
    "        \n",
    "        self.cate_cols = cate_cols\n",
    "\n",
    "        self.numeric_cols = ['Estimated_Departure_Time','Estimated_Arrival_Time','Distance']\n",
    "\n",
    "        self.max_values = X_unlabeled[self.numeric_cols].max()\n",
    "        \n",
    "        \n",
    "    # 사용 가능한 데이터 개수 return\n",
    "    def __len__(self):\n",
    "        return len(self.X_unlabeled)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # onehot encoding 후, tensor로 반환하기\n",
    "        # 1. category는 onehot으로 변환하고,\n",
    "        X_sample_category = self.X_unlabeled[self.cate_cols].iloc[i,:].to_frame().T\n",
    "        \n",
    "        X_sample_category = encoder.transform(X_sample_category)\n",
    "        \n",
    "        X_sample_category = X_sample_category.toarray()  # 추가된 코드: X_sample_category를 2차원 배열로 변환 : 희소행렬로 반환되는 onehot encoding 결과를, 일반적인 Numpy 배열로 변환해줌\n",
    "        \n",
    "        # 2.  numeric은 0~1 사이로 만들어서, category onehot 뒤에 붙이기\n",
    "        X_sample_numeric = np.array(self.X_unlabeled[self.numeric_cols].iloc[i,:]).reshape(1,-1)\n",
    "        X_sample_numeric = X_sample_numeric / self.max_values.values.reshape(1,-1)\n",
    "        \n",
    "        X_sample = np.hstack([X_sample_category,X_sample_numeric])\n",
    "        \n",
    "        # 3. 텐서로 변환하기\n",
    "        X_sample = torch.tensor(X_sample.squeeze(), dtype=torch.float32)\n",
    "        return X_sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142540 142540\n",
      "140 140\n"
     ]
    }
   ],
   "source": [
    "# 데이터 생성\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import tqdm\n",
    "\n",
    "# 데이터 가져오기\n",
    "label_dataset = Flight_labeled(X_train_labeled,y_train_labeled, encoder)\n",
    "unlabel_dataset = Flight_unlabeled(X_unlabeled, encoder)\n",
    "print(len(label_dataset),len(unlabel_dataset))\n",
    "\n",
    "# 배치화 : unlabel의 batch 개수를 label의 배치개수에 맞춰주기\n",
    "label_loader = DataLoader(label_dataset, batch_size=1024, num_workers=0)\n",
    "\n",
    "# unlabel_batch_size = len(unlabel_dataset)//len(label_loader) +1\n",
    "# print('unlabel batch size : ', unlabel_batch_size )\n",
    "unlabel_loader = DataLoader(unlabel_dataset, batch_size=1024, num_workers=0)\n",
    "print(len(label_loader),len(unlabel_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 823])\n",
      "tensor(0.) tensor(1.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_loader):\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[61], line 49\u001b[0m, in \u001b[0;36mFlight_labeled.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     45\u001b[0m         X_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_sample_category,X_sample_numeric])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#         print('5: ', X_sample.dtype, X_sample.shape)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \n\u001b[0;32m     48\u001b[0m         \u001b[38;5;66;03m# 2. 텐서로 변환하기\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m         X_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, input_dim) -> (input_dim,) 으로 차원 축소\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         y_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train_labeled\u001b[38;5;241m.\u001b[39miloc[i,:]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X_sample, y_sample\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 최소, 최대값 확인\n",
    "for batch_idx, (data, _) in enumerate(label_loader):\n",
    "    data = data.float()\n",
    "    print(data.shape)\n",
    "    print(data.min(), data.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 1/140 [00:05<12:27,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 tensor(189.)\n",
      "Data batch shape: torch.Size([1024, 823])\n",
      "Label batch shape: torch.Size([1024, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                | 2/140 [00:10<12:25,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 tensor(183.)\n",
      "Data batch shape: torch.Size([1024, 823])\n",
      "Label batch shape: torch.Size([1024, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                | 2/140 [00:14<17:04,  7.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(label_loader)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data), label\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[42], line 46\u001b[0m, in \u001b[0;36mFlight_labeled.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     42\u001b[0m         X_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_sample_category,X_sample_numeric])\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#         print('5: ', X_sample.dtype, X_sample.shape)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         \n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m# 2. 텐서로 변환하기\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m         X_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, input_dim) -> (input_dim,) 으로 차원 축소\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         y_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train_labeled\u001b[38;5;241m.\u001b[39miloc[i,:]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X_sample, y_sample\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 잘 만들어졌는지 확인\n",
    "iterator = tqdm.tqdm(label_loader)\n",
    "for data, label in iterator:\n",
    "    print(len(data), label.sum())\n",
    "    print(\"Data batch shape:\", data.shape)\n",
    "    print(\"Label batch shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기 생성\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = label_dataset[0][0].shape[0]\n",
    "hidden_dim = 256\n",
    "latent_dim = 128\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, optimizer and loss function\n",
    "device = 'cuda:0'\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "reconstruction_loss = nn.BCELoss(reduction='mean')\n",
    "\n",
    "\n",
    "# classifier 초기화\n",
    "classifier = Classifier(input_dim = latent_dim, hidden_dim = latent_dim//2 , output_dim= 2)\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, loss : 566470.9375:   1%|▊                                                    | 2/140 [00:11<12:53,  5.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(unlabel_loader)\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#         print(data.shape) # shape = [batch_size, input_dim]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m         reconstructed_data, mu, log_var \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[24], line 91\u001b[0m, in \u001b[0;36mFlight_unlabeled.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     88\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_sample_category,X_sample_numeric])\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# 3. 텐서로 변환하기\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_sample\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# unlabel Training loop 확인\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     iterator = tqdm.tqdm(unlabel_loader)\n",
    "#     for data in iterator:\n",
    "# #         print(data.shape) # shape = [batch_size, input_dim]\n",
    "#         optimizer.zero_grad()\n",
    "#         reconstructed_data, mu, log_var = model(data.to(device))\n",
    "#         loss = reconstruction_loss(reconstructed_data, data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         iterator.set_description(f\"epoch:{epoch+1}, loss : {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, loss : 163.43850708007812:   1%|▎                                           | 1/140 [00:27<1:03:43, 27.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(label_loader)\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#         print(data.shape) # shape = [batch_size, input_dim]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m         reconstructed_data, mu, log_var \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[50], line 72\u001b[0m, in \u001b[0;36mFlight_labeled.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     69\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([X_sample_category\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39msqueeze(), X_sample_numeric])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 텐서로 변환합니다.\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train_labeled\u001b[38;5;241m.\u001b[39miloc[i,:]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_sample, y_sample\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# label Training loop 확인\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    iterator = tqdm.tqdm(label_loader)\n",
    "    for data, label in iterator:\n",
    "#         print(data.shape) # shape = [batch_size, input_dim]\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_data, mu, log_var = model(data.to(device))\n",
    "        loss = reconstruction_loss(reconstructed_data, data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iterator.set_description(f\"epoch:{epoch+1}, loss : {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, labeled_loss: 0.0540984570980072, unlabeled_loss: 0.05303141474723816, class_loss: 0.7160175442695618: 100%|█|\n"
     ]
    }
   ],
   "source": [
    "# unlabeled 과 train_labeled data 같이 이용해서 신경망 훈련\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    iterator = tqdm.tqdm(zip(label_loader, unlabel_loader), total=len(label_loader))\n",
    "    \n",
    "    for (labeled_data, labeled_label), unlabeled_data in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Labeled data에 대한 VAE 훈련\n",
    "#         print(labeled_data.shape)\n",
    "        reconstructed_labeled_data, mu, log_var = model(labeled_data.to(device))\n",
    "        labeled_loss = reconstruction_loss(reconstructed_labeled_data, labeled_data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Unlabeled data에 대한 VAE 훈련\n",
    "#         print(unlabeled_data.shape)\n",
    "        reconstructed_unlabeled_data, mu, log_var = model(unlabeled_data.to(device))\n",
    "        unlabeled_loss = reconstruction_loss(reconstructed_unlabeled_data, unlabeled_data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Labeled data에 대한 Classifier 훈련\n",
    "        latent_labeled_data = model.reparameterize(mu, log_var)\n",
    "        classifier_output = classifier(latent_labeled_data)\n",
    "        labeled_label_indices = torch.argmax(labeled_label, dim=1).to(device) # 다중 타겟을 원핫으로 변환\n",
    "        class_loss = classification_loss(classifier_output, labeled_label_indices)\n",
    "        \n",
    "        # 총 Loss 계산 및 업데이트\n",
    "        total_loss = labeled_loss + unlabeled_loss + class_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iterator.set_description(f\"epoch:{epoch+1}, labeled_loss: {labeled_loss.item().round(4)}, unlabeled_loss: {unlabeled_loss.item().round(4)}, class_loss: {class_loss.item().round(4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1, total: 0.7844, labeled_loss: 0.0343, unlabeled_loss: 0.0338, class_loss:  0.7163: 100%|█| 140/140 [25:13<00:00\n"
     ]
    }
   ],
   "source": [
    "# unlabeled 과 train_labeled data 같이 이용해서 신경망 훈련\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    iterator = tqdm.tqdm(zip(label_loader, unlabel_loader), total=len(label_loader))\n",
    "    \n",
    "    for (labeled_data, labeled_label), unlabeled_data in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Labeled data에 대한 VAE 훈련\n",
    "#         print(labeled_data.shape)\n",
    "        reconstructed_labeled_data, mu, log_var = model(labeled_data.to(device))\n",
    "        labeled_loss = reconstruction_loss(reconstructed_labeled_data, labeled_data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Unlabeled data에 대한 VAE 훈련\n",
    "#         print(unlabeled_data.shape)\n",
    "        reconstructed_unlabeled_data, mu, log_var = model(unlabeled_data.to(device))\n",
    "        unlabeled_loss = reconstruction_loss(reconstructed_unlabeled_data, unlabeled_data.to(device)) - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Labeled data에 대한 Classifier 훈련\n",
    "        latent_labeled_data = model.reparameterize(mu, log_var)\n",
    "        classifier_output = classifier(latent_labeled_data)\n",
    "        labeled_label_indices = torch.argmax(labeled_label, dim=1).to(device) # 다중 타겟을 원핫으로 변환\n",
    "        class_loss = classification_loss(classifier_output, labeled_label_indices)\n",
    "        \n",
    "        # 총 Loss 계산 및 업데이트\n",
    "        total_loss = labeled_loss + unlabeled_loss + class_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iterator.set_description(f\"epoch:{epoch+1}, total:{total_loss.item() : .4f}, labeled_loss:{labeled_loss.item() : .4f}, unlabeled_loss:{unlabeled_loss.item() : .4f}, class_loss: {class_loss.item(): .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './VAE.pth') # 모델 저장\n",
    "torch.save(classifier.state_dict(), './classifier.pth') # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int64),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
       "       dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([10135, 10136, 10140, 10141, 10146, 10154, 10155, 10157, 10158,\n",
       "        10165, 10170, 10185, 10208, 10245, 10257, 10268, 10275, 10279,\n",
       "        10299, 10333, 10361, 10372, 10397, 10408, 10409, 10423, 10431,\n",
       "        10434, 10466, 10469, 10529, 10551, 10558, 10561, 10562, 10577,\n",
       "        10581, 10599, 10620, 10627, 10631, 10643, 10666, 10676, 10685,\n",
       "        10693, 10713, 10721, 10728, 10731, 10732, 10739, 10747, 10754,\n",
       "        10779, 10781, 10785, 10792, 10800, 10821, 10849, 10868, 10874,\n",
       "        10918, 10926, 10967, 10980, 10990, 10994, 11003, 11013, 11027,\n",
       "        11042, 11049, 11057, 11066, 11067, 11076, 11092, 11097, 11109,\n",
       "        11111, 11122, 11140, 11146, 11150, 11193, 11203, 11233, 11252,\n",
       "        11259, 11267, 11274, 11278, 11292, 11298, 11308, 11315, 11336,\n",
       "        11337, 11413, 11415, 11423, 11433, 11445, 11447, 11468, 11470,\n",
       "        11471, 11481, 11503, 11525, 11537, 11540, 11577, 11587, 11603,\n",
       "        11612, 11617, 11618, 11624, 11630, 11637, 11638, 11641, 11648,\n",
       "        11695, 11697, 11699, 11721, 11775, 11778, 11823, 11865, 11867,\n",
       "        11884, 11898, 11905, 11921, 11953, 11973, 11977, 11980, 11982,\n",
       "        11986, 11995, 11996, 11997, 12003, 12007, 12012, 12016, 12094,\n",
       "        12119, 12124, 12129, 12156, 12173, 12177, 12191, 12197, 12206,\n",
       "        12217, 12223, 12244, 12250, 12255, 12264, 12265, 12266, 12278,\n",
       "        12280, 12323, 12335, 12339, 12343, 12365, 12389, 12391, 12397,\n",
       "        12402, 12441, 12448, 12451, 12478, 12492, 12511, 12519, 12523,\n",
       "        12544, 12758, 12819, 12884, 12888, 12889, 12891, 12892, 12896,\n",
       "        12898, 12899, 12902, 12915, 12917, 12945, 12951, 12953, 12954,\n",
       "        12982, 12992, 13029, 13034, 13061, 13076, 13121, 13127, 13139,\n",
       "        13158, 13184, 13198, 13204, 13230, 13232, 13241, 13244, 13256,\n",
       "        13264, 13277, 13290, 13296, 13303, 13342, 13344, 13347, 13360,\n",
       "        13367, 13377, 13388, 13422, 13433, 13459, 13476, 13485, 13486,\n",
       "        13487, 13495, 13502, 13541, 13577, 13795, 13796, 13829, 13830,\n",
       "        13832, 13851, 13871, 13873, 13891, 13930, 13931, 13933, 13964,\n",
       "        13970, 13983, 14004, 14006, 14025, 14027, 14057, 14082, 14092,\n",
       "        14098, 14100, 14107, 14108, 14109, 14112, 14113, 14120, 14122,\n",
       "        14150, 14193, 14222, 14231, 14237, 14252, 14254, 14256, 14259,\n",
       "        14262, 14288, 14303, 14307, 14314, 14321, 14457, 14487, 14489,\n",
       "        14492, 14512, 14520, 14524, 14543, 14570, 14574, 14576, 14582,\n",
       "        14588, 14633, 14635, 14674, 14679, 14683, 14685, 14689, 14696,\n",
       "        14698, 14704, 14709, 14711, 14716, 14730, 14747, 14761, 14771,\n",
       "        14783, 14794, 14802, 14814, 14828, 14831, 14842, 14843, 14869,\n",
       "        14877, 14893, 14905, 14908, 14952, 14955, 14960, 14986, 15008,\n",
       "        15016, 15023, 15024, 15027, 15041, 15048, 15070, 15074, 15096,\n",
       "        15249, 15295, 15304, 15323, 15356, 15370, 15376, 15380, 15389,\n",
       "        15401, 15411, 15412, 15454, 15582, 15607, 15624, 15841, 15897,\n",
       "        15919, 15991, 16101, 16218, 16869], dtype=int64),\n",
       " array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "        'Colorado', 'Connecticut', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
       "        'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
       "        'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "        'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
       "        'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
       "        'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "        'Pennsylvania', 'Puerto Rico', 'Rhode Island', 'South Carolina',\n",
       "        'South Dakota', 'Tennessee', 'Texas',\n",
       "        'U.S. Pacific Trust Territories and Possessions',\n",
       "        'U.S. Virgin Islands', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "        'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object),\n",
       " array([10135, 10136, 10140, 10141, 10146, 10154, 10155, 10157, 10158,\n",
       "        10165, 10170, 10185, 10208, 10245, 10257, 10268, 10275, 10279,\n",
       "        10299, 10333, 10361, 10372, 10397, 10408, 10409, 10423, 10431,\n",
       "        10434, 10466, 10469, 10529, 10551, 10558, 10561, 10562, 10577,\n",
       "        10581, 10599, 10620, 10627, 10631, 10643, 10666, 10676, 10685,\n",
       "        10693, 10713, 10721, 10728, 10731, 10732, 10739, 10747, 10754,\n",
       "        10779, 10781, 10785, 10792, 10800, 10821, 10849, 10868, 10874,\n",
       "        10918, 10926, 10967, 10980, 10990, 10994, 11003, 11013, 11027,\n",
       "        11042, 11049, 11057, 11066, 11067, 11076, 11092, 11097, 11109,\n",
       "        11111, 11122, 11140, 11146, 11150, 11193, 11203, 11233, 11252,\n",
       "        11259, 11267, 11274, 11278, 11292, 11298, 11308, 11315, 11336,\n",
       "        11337, 11413, 11415, 11423, 11433, 11445, 11447, 11468, 11470,\n",
       "        11471, 11481, 11503, 11525, 11537, 11540, 11577, 11587, 11603,\n",
       "        11612, 11617, 11618, 11624, 11630, 11637, 11638, 11641, 11648,\n",
       "        11695, 11697, 11699, 11721, 11775, 11778, 11823, 11865, 11867,\n",
       "        11884, 11898, 11905, 11921, 11953, 11973, 11977, 11980, 11982,\n",
       "        11986, 11995, 11996, 11997, 12003, 12007, 12012, 12016, 12094,\n",
       "        12119, 12124, 12129, 12156, 12173, 12177, 12191, 12197, 12206,\n",
       "        12217, 12223, 12244, 12250, 12255, 12264, 12265, 12266, 12278,\n",
       "        12280, 12323, 12335, 12339, 12343, 12365, 12389, 12391, 12397,\n",
       "        12402, 12441, 12448, 12451, 12478, 12492, 12511, 12519, 12523,\n",
       "        12544, 12758, 12819, 12884, 12888, 12889, 12891, 12892, 12896,\n",
       "        12898, 12899, 12902, 12915, 12917, 12945, 12951, 12953, 12954,\n",
       "        12982, 12992, 13029, 13034, 13061, 13076, 13121, 13127, 13139,\n",
       "        13158, 13184, 13198, 13204, 13230, 13232, 13241, 13244, 13256,\n",
       "        13264, 13277, 13290, 13296, 13303, 13342, 13344, 13347, 13360,\n",
       "        13367, 13377, 13388, 13422, 13433, 13459, 13476, 13485, 13486,\n",
       "        13487, 13495, 13502, 13541, 13577, 13795, 13796, 13829, 13830,\n",
       "        13832, 13851, 13871, 13873, 13891, 13930, 13931, 13933, 13964,\n",
       "        13970, 13983, 14004, 14006, 14025, 14027, 14057, 14082, 14092,\n",
       "        14098, 14100, 14107, 14108, 14109, 14112, 14113, 14120, 14122,\n",
       "        14150, 14193, 14222, 14231, 14237, 14252, 14254, 14256, 14259,\n",
       "        14262, 14288, 14303, 14307, 14314, 14321, 14457, 14487, 14489,\n",
       "        14492, 14512, 14520, 14524, 14543, 14570, 14574, 14576, 14582,\n",
       "        14588, 14633, 14635, 14674, 14679, 14683, 14685, 14689, 14696,\n",
       "        14698, 14704, 14709, 14711, 14716, 14730, 14747, 14761, 14771,\n",
       "        14783, 14794, 14802, 14814, 14828, 14831, 14842, 14843, 14869,\n",
       "        14877, 14893, 14905, 14908, 14952, 14955, 14960, 14986, 15008,\n",
       "        15016, 15023, 15024, 15027, 15041, 15048, 15070, 15074, 15096,\n",
       "        15249, 15295, 15304, 15323, 15356, 15370, 15376, 15380, 15389,\n",
       "        15401, 15411, 15412, 15454, 15582, 15607, 15624, 15841, 15897,\n",
       "        15919, 15991, 16101, 16133, 16218, 16869], dtype=int64),\n",
       " array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "        'Colorado', 'Connecticut', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
       "        'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
       "        'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "        'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
       "        'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
       "        'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "        'Pennsylvania', 'Puerto Rico', 'Rhode Island', 'South Carolina',\n",
       "        'South Dakota', 'Tennessee', 'Texas',\n",
       "        'U.S. Pacific Trust Territories and Possessions',\n",
       "        'U.S. Virgin Islands', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "        'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object),\n",
       " array(['Air Wisconsin Airlines Corp', 'Alaska Airlines Inc.',\n",
       "        'Allegiant Air', 'American Airlines Inc.', 'Cape Air',\n",
       "        'Capital Cargo International', 'Comair Inc.',\n",
       "        'Commutair Aka Champlain Enterprises, Inc.', 'Compass Airlines',\n",
       "        'Delta Air Lines Inc.', 'Empire Airlines Inc.',\n",
       "        'Endeavor Air Inc.', 'Envoy Air', 'ExpressJet Airlines Inc.',\n",
       "        'Frontier Airlines Inc.',\n",
       "        'GoJet Airlines, LLC d/b/a United Express',\n",
       "        'Hawaiian Airlines Inc.', 'Horizon Air', 'JetBlue Airways',\n",
       "        'Mesa Airlines Inc.', 'Peninsula Airways Inc.',\n",
       "        'Republic Airlines', 'SkyWest Airlines Inc.',\n",
       "        'Southwest Airlines Co.', 'Spirit Air Lines',\n",
       "        'Trans States Airlines', 'United Air Lines Inc.', 'Virgin America'],\n",
       "       dtype=object),\n",
       " array(['AA', 'AS', 'B6', 'DL', 'F9', 'G4', 'HA', 'NK', 'UA', 'VX', 'WN'],\n",
       "       dtype=object),\n",
       " array([19393., 19687., 19690., 19790., 19805., 19930., 19977., 20046.,\n",
       "        20225., 20237., 20253., 20263., 20304., 20363., 20366., 20368.,\n",
       "        20378., 20397., 20398., 20409., 20416., 20427., 20436., 20445.,\n",
       "        20452., 20500., 21167., 21171.]),\n",
       " array(['215NV', '216NV', '217NV', ..., 'N999DN', 'N999JB', 'N999JQ'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.categories_\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래는 아직 작성 안한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999950</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999955</th>\n",
       "      <td>Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999963</th>\n",
       "      <td>Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999985</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999992</th>\n",
       "      <td>Not_Delayed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178176 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Delay\n",
       "6       Not_Delayed\n",
       "8       Not_Delayed\n",
       "10          Delayed\n",
       "12      Not_Delayed\n",
       "13      Not_Delayed\n",
       "...             ...\n",
       "999950  Not_Delayed\n",
       "999955      Delayed\n",
       "999963      Delayed\n",
       "999985  Not_Delayed\n",
       "999992  Not_Delayed\n",
       "\n",
       "[178176 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSpy0yeMpkYc"
   },
   "source": [
    "## 2. 준지도학습진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJwR7tZ0qtxd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkrYr_Q2kaa-"
   },
   "source": [
    "### 전처리방법7 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrfZWy4skaa_",
    "outputId": "4ab0d8cf-c561-46a7-99ee-d94fa4379fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번째 전처리 방법인지 정수-정수를 입력하세요 : 4\n"
     ]
    }
   ],
   "source": [
    "save_idx = input('몇 번째 전처리 방법인지 정수-정수를 입력하세요 : ')\n",
    "train_save_name = 'train_preprocess_' + save_idx\n",
    "test_save_name = 'test_preprocess_' + save_idx\n",
    "train.to_parquet(f'./data/{train_save_name}.parquet')\n",
    "test.to_parquet(f'./data/{test_save_name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjmnbSg0kabA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
